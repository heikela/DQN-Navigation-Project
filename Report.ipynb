{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains my solution to the navigation challenge described in the Readme. To define and retrain the agent, execute all code cells below.\n",
    "\n",
    "If further evaluation of the successfully trained agent is desired, this can be achieved by running the code that defines the model, skipping over the training, creating an agent and replacing its local and target network weights with weights loaded from the checkpoint file (code for this is not included).\n",
    "\n",
    "### 1. Set up the Environment\n",
    "\n",
    "Install dependencies by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the Unity Environment that defines the environment for our learning task. If running this notebook on the Jupyter environment of the Udacity course, execute the following cell as is - the environment is preinstalled in this case. If running locally, you may need to edit the `file_name` to match your installation location and the operating system dependent filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# please do not modify the line below if running in the Udacity environment\n",
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. State and Action Spaces\n",
    "\n",
    "Let us examine the state and action spaces provided by the environment to check that we have understood the environment description correctly and are ready to define an agent that makes use of the available observations and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [ 1.          0.          0.          0.          0.84408134  0.          0.\n",
      "  1.          0.          0.0748472   0.          1.          0.          0.\n",
      "  0.25755     1.          0.          0.          0.          0.74177343\n",
      "  0.          1.          0.          0.          0.25854847  0.          0.\n",
      "  1.          0.          0.09355672  0.          1.          0.          0.\n",
      "  0.31969345  0.          0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are four available actions as expected, and that states have a length of 37, also as expected based on documentation. It is not at all obvious what the structure of the observation vector is, but perhaps an agent can learn to make use of it through reinforcement learning even if its programmer does not understand the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define an artificial neural network that can be used as a Q estimator in this environment, and a replay buffer for reusing past experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a neural network with 2 hidden layers to represent our estimates for the Q values of the optimal policy.\n",
    "\n",
    "The network is set up so that given a state/observation, it will simultaneusly output the Q value estimates of each action. Because of this, the output layer size needs to match the number of available actions in the environment.\n",
    "\n",
    "Similarly the size of the input to the network should match the size of the observed state (after preprocessing if any).\n",
    "\n",
    "The sizes of the hidden layers can also be given as parameters. I have chosen [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) as the nonlinearity after each hidden layer. There is no nonlinearity after the final fully connected layer as the ability to represent arbitrary Q-values is desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork3Layer(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, nh1=64, nh2=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork3Layer, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.FC1 = nn.Linear(state_size, nh1)\n",
    "        self.FC2 = nn.Linear(nh1, nh2)\n",
    "        self.Output = nn.Linear(nh2, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        return self.Output(F.relu(self.FC2(F.relu(self.FC1(state)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to employ the Deep Q-Learning algorithm, we will need a replay buffer for tracking past experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Agent definition\n",
    "\n",
    "Equipped with a neural network that could potentially learn to estimate Q-values and a replay buffer to help us re-use experience in the Deep Q-Learning algorithm, we are ready to define an agent that combines these to explore and learn to behave more effectively in the given environment.\n",
    "\n",
    "The agent maintains a replay buffer for past training samples, and two Q-Networks. When learning, the weights of the `local` network are optimized, using `target` network together with rewards to set the target for learning. Weights of the target network are updated to follow the local weights slowly.\n",
    "\n",
    "For updating the local weights, Adam optimizer is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on whether Cuda is available, place tensor computations on the GPU or CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 lr=5e-4,\n",
    "                 update_every=1,\n",
    "                 batches_per_update=1):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "            buffer_size (int): how many steps of past experience to retain\n",
    "            batch_size (int): how many samples from experience to use in one minibatch of training\n",
    "            gamma: discount rate for rewards\n",
    "            tau: the rate at which weights are transferred from the currently optimized Q-network to the target network\n",
    "            lr: learning rate\n",
    "            update_every: update network weights after this many timesteps\n",
    "            batches_per_update: how many weight updates to carry out each time training occurs (how many minibatches and\n",
    "              transfers of weights from current to target network)\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.update_every = update_every\n",
    "        self.batches_per_update = batches_per_update\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork3Layer(self.state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork3Layer(self.state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(buffer_size, batch_size, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Keep track of experience, and learn.\"\"\"\n",
    "        # Save experience in replay buffer\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every update_every time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                # Carry out batches_per_update updates.\n",
    "                for i in range(0, self.batches_per_update):\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnetwork_local(state)\n",
    "                return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## compute and minimize the loss\n",
    "        # set networks to training mode in case any layers have mode dependent behaviour.\n",
    "        self.qnetwork_local.train()\n",
    "        self.qnetwork_target.train()\n",
    "        with torch.no_grad():\n",
    "            next_qs = self.qnetwork_target.forward(next_states)\n",
    "            max_next_qs = next_qs.max(1, keepdim=True)[0]\n",
    "            targets = (1 - dones) * self.gamma * max_next_qs + rewards\n",
    "        self.optimizer.zero_grad()\n",
    "        prediction = self.qnetwork_local.forward(states).gather(1, actions)\n",
    "        loss = F.mse_loss(prediction, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the agent with deep Q-learning\n",
    "\n",
    "Define a function to carry out the [deep Q-learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) algorithm.\n",
    "\n",
    "Because the ray-based observation might miss bananas that fall between rays when the agent turns, we include preprocessing which concatenates the four most recent observations and presents the agent with this combined vector as the state / observation vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(agent, max_episode=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, previous_scores = None):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        agent (Agent): the agent to train\n",
    "        max_episodes (int): maximum number of training episodes, including any episodes supplied in previous_scores\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        previous_scores (array of float): scores from earlier training phases.\n",
    "    \"\"\"\n",
    "    scores = previous_scores if previous_scores else []           # list containing scores from each episode\n",
    "    scores_window = deque(scores, maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(len(scores) + 1, max_episode+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state_frame = env_info.vector_observations[0]            # get the current state\n",
    "        # we want to preprocess the state to concatenate 4 most recent observations.\n",
    "        # Initialise with 4 times the initial observation.\n",
    "        state = np.hstack([state_frame, state_frame, state_frame, state_frame])\n",
    "        score = 0\n",
    "        for t in range(0, max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state_frame = env_info.vector_observations[0]   # get the next state\n",
    "            next_state = np.hstack([state[1:-state_size + 1], next_state_frame]) # concatenate four most recent states\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a grid search over a few different parameter choices to understand which hyperparameters look promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = namedtuple(\"Params\", field_names=[\"lr\", \"tau\", \"frames_between_update\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Params(lr=0.001, tau=0.001, frames_between_update=1),\n",
       " Params(lr=0.001, tau=0.001, frames_between_update=4),\n",
       " Params(lr=0.001, tau=0.0001, frames_between_update=1),\n",
       " Params(lr=0.001, tau=0.0001, frames_between_update=4),\n",
       " Params(lr=0.0003, tau=0.001, frames_between_update=1),\n",
       " Params(lr=0.0003, tau=0.001, frames_between_update=4),\n",
       " Params(lr=0.0003, tau=0.0001, frames_between_update=1),\n",
       " Params(lr=0.0003, tau=0.0001, frames_between_update=4),\n",
       " Params(lr=0.0001, tau=0.001, frames_between_update=1),\n",
       " Params(lr=0.0001, tau=0.001, frames_between_update=4),\n",
       " Params(lr=0.0001, tau=0.0001, frames_between_update=1),\n",
       " Params(lr=0.0001, tau=0.0001, frames_between_update=4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_grid = [params(lr, tau, frames_between_update)\n",
    "               for lr in [1e-3, 3e-4, 1e-4]\n",
    "               for tau in [1e-3, 1e-4]\n",
    "               for frames_between_update in [1, 4]]\n",
    "params_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a limited length training procedure at each hyperparameter combination defined above. Use starting epsilon of 0.5. In initial testing it appeared that very high epsilon values did not allow any learning to happen, presumably because the agent was not moving forward enough to encounter bananas at all in most test runs.\n",
    "\n",
    "We use the default hidden layer sizes as defined by the neural network definition above. This means 64 hidden units in each of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [Agent(state_size=state_size * 4, action_size=action_size, seed=4, lr=p.lr, tau=p.tau, update_every=p.frames_between_update)\n",
    "          for p in params_grid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.37\n",
      "Episode 200\tAverage Score: 0.95\n",
      "Episode 100\tAverage Score: 1.71\n",
      "Episode 200\tAverage Score: 4.29\n",
      "Episode 100\tAverage Score: 1.38\n",
      "Episode 200\tAverage Score: 3.05\n",
      "Episode 100\tAverage Score: 0.08\n",
      "Episode 200\tAverage Score: 1.16\n",
      "Episode 100\tAverage Score: 1.73\n",
      "Episode 200\tAverage Score: 4.18\n",
      "Episode 100\tAverage Score: 2.05\n",
      "Episode 200\tAverage Score: 5.58\n",
      "Episode 100\tAverage Score: 0.62\n",
      "Episode 200\tAverage Score: 2.24\n",
      "Episode 100\tAverage Score: 0.55\n",
      "Episode 200\tAverage Score: 0.67\n",
      "Episode 100\tAverage Score: 2.19\n",
      "Episode 200\tAverage Score: 6.41\n",
      "Episode 100\tAverage Score: -0.01\n",
      "Episode 200\tAverage Score: 1.28\n",
      "Episode 100\tAverage Score: 0.82\n",
      "Episode 200\tAverage Score: 2.68\n",
      "Episode 100\tAverage Score: 0.56\n",
      "Episode 200\tAverage Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "scores = [dqn(agent, eps_start = 0.5, eps_end = 0.05, eps_decay = 0.997, max_episode=200) for agent in agents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the 9th agent performs best in this trial. Let us continue training this one with epsilon continuing from where it would be left after the 200 episodes above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300\tAverage Score: 7.65\n",
      "Episode 400\tAverage Score: 8.42\n",
      "Episode 500\tAverage Score: 10.15\n",
      "Episode 600\tAverage Score: 10.86\n",
      "Episode 700\tAverage Score: 10.93\n",
      "Episode 800\tAverage Score: 11.08\n",
      "Episode 900\tAverage Score: 11.33\n",
      "Episode 1000\tAverage Score: 11.33\n",
      "Episode 1100\tAverage Score: 11.12\n",
      "Episode 1200\tAverage Score: 10.86\n",
      "Episode 1300\tAverage Score: 11.59\n",
      "Episode 1400\tAverage Score: 11.97\n",
      "Episode 1500\tAverage Score: 11.76\n",
      "Episode 1600\tAverage Score: 11.32\n",
      "Episode 1700\tAverage Score: 12.32\n",
      "Episode 1800\tAverage Score: 12.35\n",
      "Episode 1900\tAverage Score: 11.81\n",
      "Episode 2000\tAverage Score: 12.08\n",
      "Episode 2100\tAverage Score: 11.88\n",
      "Episode 2200\tAverage Score: 11.11\n",
      "Episode 2300\tAverage Score: 11.69\n",
      "Episode 2400\tAverage Score: 12.47\n",
      "Episode 2500\tAverage Score: 12.90\n",
      "Episode 2504\tAverage Score: 13.00\n",
      "Environment solved in 2404 episodes!\tAverage Score: 13.00\n"
     ]
    }
   ],
   "source": [
    "best_agent = agents[8]\n",
    "cont_scores = dqn(best_agent, eps_start = 0.275, eps_end = 0.1, eps_decay = 0.997, max_episode=5000, previous_scores = scores[8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Results\n",
    "\n",
    "As we can see in the output above, the agent trained with the hyperparameters that seemed most promising solved the invironment in 2404 steps when we continued the training. The model weights at the end of the evaluation have now been saved in the file `checkpoint.pth` and can be loaded from there if any later evaluation is desired. Finally, let us plot the scores to see how the agent improved its performance over the training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a2e8f8400>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(cont_scores)), cont_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that learning does not proceed very effectively in this case after reaching an average return of approximately 10 per episode in 500 episodes.\n",
    "\n",
    "To finish off, we can close the Unity environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Further work\n",
    "\n",
    "The most obvious short term avenue for further work would be to investigate why learning slowed down around average return levels of 10 to 11, after 500 episodes. Running multiple training runs or trying different hyperparameters at this stage of the training could shed light into reasons of slow learning.\n",
    "\n",
    "Additionally, further work could proceed in at least three broad directions: Optimizing the agent's structure, using a more sophisticated learning algorithm, or solving the environment based on pixel input instead of the ray-based observations used in this version.\n",
    "\n",
    "#### Optimizing the agent structure\n",
    "\n",
    "It would be interesting to see whether quicker learning results can be achieved with a different neural network architecture or preprocessing. For example, a more powerful neural network could be created by adding another hidden layer. As the neural network becomes more expressive, the risk of overfitting to observations increases. This could be countered e.g. by including dropout regularisation in the network. Equally, it would be interesting to see if adequate results can be achieved with an even smaller neural network and less preprocessing.\n",
    "\n",
    "#### More sophisticated learning algorithm\n",
    "\n",
    "The simple DQN algorithm could be improved by implementing:\n",
    " - [Double Q Learning](https://papers.nips.cc/paper/3964-double-q-learning), reducing the tendency for a SARSAMAX (like Deep Q-learning) algorithm to overestimate Q values.\n",
    " - [Dueling Networks](https://arxiv.org/abs/1511.06581) where instead of the action value function Q, a network with two \"heads\" estimates the state value function V and the action depended advantage function A separately, resulting in lower variance\n",
    " - [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952) which makes better use of recorded experience by focusing on samples that the network has trouble estimating correctly.\n",
    "\n",
    "#### Pixel based version of the environment\n",
    "\n",
    "A version of the environment with pixel based first person view is available. Solving this would require a larger neural network, but potentially allow for better solutions due to more information available for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
